{
 "metadata": {
  "name": "",
  "signature": "sha256:608c370b896fccb0b72a3fb89bd7dc1efc1408037efd4e6c53a2fa82e4539644"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn import cross_validation\n",
      "from sklearn.ensemble import RandomForestClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load ../../ocdata_categorize/words.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load ../../ocdata_categorize/samples.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_data = {\n",
      "    'Keywords': '../../data/pycon-sprint/keywords.csv',\n",
      "    'UK': '../../data/pycon-sprint/UK.csv',\n",
      "    'Georgia': '../../data/pycon-sprint/Georgia.csv',\n",
      "    'Mexico': '../../data/pycon-sprint/Mexico.csv',\n",
      "    'EU': '../../data/pycon-sprint/EU.csv',\n",
      "    'Canada': '../../data/pycon-sprint/Canada.csv',\n",
      "    'Moldova':'../../data/pycon-sprint/Moldova.csv',\n",
      "    'UNOPS': '../../data/pycon-sprint/UNOPS.csv',\n",
      "}\n",
      "\n",
      "original_key = 'English Name'\n",
      "entity_key = 'Entity'\n",
      "\n",
      "\n",
      "\n",
      "data = load_samples(raw_data, original_key, entity_key)\n",
      "keywords = load_samples({'Keywords': raw_data['Keywords']}, original_key, entity_key)\n",
      "entities = list( set(x['entity'] for x in keywords) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "slices = {}\n",
      "for i, row in enumerate(data):\n",
      "    slices.setdefault(row['sample'], []).append(i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 137
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "I. Define Raw Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def organize_data(data):\n",
      "    organized = []\n",
      "    for k, headers in data.items():\n",
      "        for header in headers:\n",
      "            organized.append({'entity': k, 'header': header})\n",
      "    return organized\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 138
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "II. Define Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def length(df):\n",
      "    return df['header'].apply(len)\n",
      "\n",
      "def word_count(df):\n",
      "    return df['header'].apply(lambda x: len(list(split_words(x))))\n",
      "\n",
      "def header_in_entity(df):\n",
      "    return df['header'].str.lower().isin(df['entity'].str.lower())\n",
      "\n",
      "def entity_in_header(df):\n",
      "    return df['entity'].str.lower().isin(df['header'].str.lower())\n",
      "\n",
      "def entity_feature(name):\n",
      "    entity_set = set(x['header'] for x in data if x['entity'] == name)\n",
      "    \n",
      "    def fn(x):\n",
      "        #print name, x, words.subsetness(x, entity_set)\n",
      "        try:\n",
      "            return words.subsetness(x, entity_set)\n",
      "        except:\n",
      "            return 0\n",
      "    \n",
      "    def entity_feature(df):\n",
      "        return df['header'].apply( fn )\n",
      "    \n",
      "    entity_feature.func_name = 'entity_%s' % name    \n",
      "    return entity_feature\n",
      "\n",
      "entity_features = [entity_feature(name) for name in entities]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "III. Combine Features into Feature Matrix & Define Outcome"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "IV. Create Model"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "V. Split Data into Test and Training"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Fit and Test Models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Model(object):\n",
      "    def __init__(self, samples, outcome_key='entity', svm=RandomForestClassifier(n_estimators=10)):\n",
      "        self.samples = samples\n",
      "        self.svm = svm\n",
      "        self.frame = pd.DataFrame(self.samples)\n",
      "        self.outcome_key = outcome_key\n",
      "        self.features_built = set()\n",
      "    \n",
      "    def test(self, features, iterations=5, train_size=0.35, test_size=.25, seed=0): \n",
      "        X = self.build(self.frame, features)\n",
      "        y = self.frame[self.outcome_key]\n",
      "        \n",
      "        rs = cross_validation.ShuffleSplit(len(X), n_iter=iterations, train_size=train_size, test_size=test_size, random_state=seed)\n",
      "        \n",
      "        accuracies = []\n",
      "        for train_index, test_index in rs:\n",
      "            model = self.svm.fit(X.ix[train_index], y.ix[train_index])\n",
      "            actual = y.ix[test_index].values\n",
      "            predicted = model.predict(X.ix[test_index])\n",
      "            accuracies.append( self.score_model(actual, predicted) )\n",
      "        \n",
      "        print \"Avg Accuracy: %%%.2f\" % np.mean(accuracies)\n",
      "    \n",
      "    def test_sample(self, slice, features):\n",
      "        X = self.build(self.frame, features)\n",
      "        y = self.frame[self.outcome_key]\n",
      "        \n",
      "        model = self.svm.fit(X, y)\n",
      "        actual = y.ix[slice].values\n",
      "        predicted = model.predict(X.ix[slice])\n",
      "        accuracy = self.score_model(actual, predicted)\n",
      "        \n",
      "        for i, a, p in zip(slice, actual, predicted):\n",
      "            print self.samples[i]['header'].ljust(50), a.ljust(20), p\n",
      "        \n",
      "        print \"Accuracy: %%%.2f\" % accuracy\n",
      "    \n",
      "    def test_data(self, data, features):\n",
      "        X = self.build(self.frame, features)\n",
      "        y = self.frame[self.outcome_key]\n",
      "        \n",
      "        model = self.svm.fit(X, y)\n",
      "        \n",
      "        df = pd.DataFrame(data)\n",
      "        z = self.build(df, features)\n",
      "        \n",
      "        actual = df.entity\n",
      "        predicted = model.predict(z)\n",
      "        accuracy = self.score_model(actual, predicted)\n",
      "        \n",
      "        for dct, a, p in zip(data, actual, predicted):\n",
      "            print dct['header'].ljust(50), a.ljust(20), p\n",
      "        \n",
      "        print \"Accuracy: %%%.2f\" % accuracy\n",
      "    \n",
      "    def score_model(self, actual, predicted):\n",
      "        score_df = pd.DataFrame([actual, predicted], index=['actual', 'predicted']).T\n",
      "        correct = sum(score_df.actual == score_df.predicted)\n",
      "        incorrect = sum(score_df.actual != score_df.predicted)\n",
      "        total = correct + incorrect\n",
      "        accuracy = float(correct) / float(total) * 100\n",
      "        return accuracy\n",
      "    \n",
      "    def predict(self, headers, features):\n",
      "        X = self.build(self.frame, features)\n",
      "        y = self.frame[self.outcome_key]\n",
      "        model = self.svm.fit(X, y)\n",
      "        \n",
      "        data = [{'header': h, 'entity': '?'} for h in headers]\n",
      "        df = pd.DataFrame(data)\n",
      "        z = self.build(df, features)\n",
      "        \n",
      "        self.df = df\n",
      "        self.z = z\n",
      "        \n",
      "        predictions = model.predict(z)\n",
      "        return zip(headers, predictions)\n",
      "    \n",
      "    def build(self, df, features):\n",
      "        result = pd.DataFrame()\n",
      "        for fn in features:\n",
      "            result[fn.func_name] = fn(df)\n",
      "        return result\n",
      "\n",
      "model = Model(data)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.test(features=[length, word_count])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Avg Accuracy: %34.46\n"
       ]
      }
     ],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.test(features=[length, word_count, header_in_entity, entity_in_header])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Avg Accuracy: %50.30\n"
       ]
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.test(features=[length, word_count, header_in_entity, entity_in_header] + entity_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Avg Accuracy: %49.50\n"
       ]
      }
     ],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.test(features=entity_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Avg Accuracy: %34.46\n"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Predict a dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.test_sample(slices['UK'], features=[length, word_count, header_in_entity, entity_in_header] + entity_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NOTICEID                                           notice               notice\n",
        "REFERENCENUMBER                                    ?                    ?\n",
        "DATEPUBLISHED                                      solicitation         solicitation\n",
        "VALUEMIN                                           solicitation         solicitation\n",
        "VALUEMAX                                           solicitation         solicitation\n",
        "STATUS                                             solicitation         solicitation\n",
        "URL                                                notice               notice\n",
        "ORG_NAME                                           buyer                buyer\n",
        "ORG_CONTACTEMAIL                                   buyer                solicitation\n",
        "TITLE                                              good                 contract\n",
        "DESCRIPTION                                        good                 good\n",
        "NOTICETYPE                                         notice               contract\n",
        "REGION                                             buyer                buyer\n",
        "NOTICE_STATE                                       notice               notice\n",
        "NOTICE_STATE_CHANGE_DATE                           notice               notice\n",
        "CLASSIFICATION                                     good                 notice\n",
        "NUM_DOCS                                           notice               notice\n",
        "Accuracy: %76.47\n"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = Model(keywords)\n",
      "predict_data = list(item for item in data if item['sample']=='UK')\n",
      "results = model.predict(predict_data, features=[length, word_count, header_in_entity, entity_in_header] + entity_features)\n",
      "\n",
      "for input, result in results:\n",
      "    print input['header'].ljust(50), result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NOTICEID                                           solicitation\n",
        "REFERENCENUMBER                                    solicitation\n",
        "DATEPUBLISHED                                      solicitation\n",
        "VALUEMIN                                           solicitation\n",
        "VALUEMAX                                           solicitation\n",
        "STATUS                                             solicitation\n",
        "URL                                                solicitation\n",
        "ORG_NAME                                           solicitation\n",
        "ORG_CONTACTEMAIL                                   solicitation\n",
        "TITLE                                              solicitation\n",
        "DESCRIPTION                                        solicitation\n",
        "NOTICETYPE                                         solicitation\n",
        "REGION                                             solicitation\n",
        "NOTICE_STATE                                       solicitation\n",
        "NOTICE_STATE_CHANGE_DATE                           solicitation\n",
        "CLASSIFICATION                                     solicitation\n",
        "NUM_DOCS                                           solicitation\n"
       ]
      }
     ],
     "prompt_number": 146
    }
   ],
   "metadata": {}
  }
 ]
}